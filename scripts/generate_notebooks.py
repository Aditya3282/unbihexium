"""Generate 130 topic-based notebooks for Unbihexium model zoo."""

import json
import os
from pathlib import Path

# Model topics (130 total)
TOPICS = [
    "accessibility_analyzer", "aircraft_detector", "asset_condition_change", "beekeeping_suitability",
    "border_monitor", "building_detector", "builtup_detector", "business_valuation", "change_detector",
    "cloud_mask", "construction_monitor", "coregistration", "corridor_monitor", "crop_boundary_delineation",
    "crop_classifier", "crop_detector", "crop_growth_monitor", "crop_health_assessor", "damage_assessor",
    "deforestation_detector", "dem_generator", "desertification_monitor", "digitization_2d", "digitization_3d",
    "disaster_management", "drought_monitor", "dsm_generator", "dtm_generator", "economic_spatial_assessor",
    "emergency_disaster_manager", "encroachment_detector", "energy_potential", "environmental_monitor",
    "environmental_risk", "erosion_detector", "evi_calculator", "field_surveyor", "fire_monitor",
    "flood_risk", "flood_risk_assessor", "forest_density_estimator", "forest_monitor", "geostatistical_analyzer",
    "grazing_potential", "greenhouse_detector", "ground_displacement", "hazard_vulnerability",
    "hydroelectric_monitor", "infrastructure_monitor", "insurance_underwriting", "land_degradation_detector",
    "land_surface_temperature", "landslide_risk", "leakage_detector", "livestock_estimator", "lulc_classifier",
    "marine_pollution_detector", "maritime_awareness", "military_objects_detector", "mobility_analyzer",
    "model_3d", "mosaic_processor", "mosaicking", "msi_calculator", "multi_solution_segmentation",
    "multispectral_processor", "natural_resources_monitor", "nbr_calculator", "ndvi_calculator",
    "ndwi_calculator", "network_analyzer", "object_detector", "offshore_survey", "onshore_monitor",
    "ortho_processor", "orthorectification", "panchromatic_processor", "pansharpening",
    "perennial_garden_suitability", "pipeline_route_planner", "pivot_inventory", "plowed_land_detector",
    "preparedness_manager", "protected_area_change_detector", "raster_tiler", "reservoir_monitor",
    "resource_allocation", "road_network_analyzer", "route_planner", "salinity_detector", "sar_amplitude",
    "sar_flood_detector", "sar_mapping_workflow", "sar_oil_spill_detector", "sar_phase_displacement",
    "sar_ship_detector", "sar_subsidence_monitor", "savi_calculator", "security_monitor", "seismic_risk",
    "ship_detector", "site_suitability", "solar_site_selector", "spatial_analyzer", "spatial_relationship",
    "stereo_processor", "super_resolution", "synthetic_imagery", "target_detector", "thematic_mapper",
    "timeseries_analyzer", "topography_mapper", "tourist_destination_monitor", "transportation_mapper",
    "tree_height_estimator", "tri_stereo_processor", "urban_growth_assessor", "urban_planner",
    "utility_mapper", "vegetation_condition", "vehicle_detector", "viewshed_analyzer",
    "water_quality_assessor", "water_surface_detector", "watershed_manager", "wildfire_risk",
    "wildlife_habitat_analyzer", "wind_site_selector", "yield_predictor", "zonal_statistics"
]

# Model descriptions
DESCRIPTIONS = {
    "accessibility_analyzer": ("Accessibility Analyzer", "Spatial accessibility and reachability analysis for urban planning and emergency services", "regression"),
    "aircraft_detector": ("Aircraft Detector", "Detection and classification of aircraft from satellite and aerial imagery", "detection"),
    "asset_condition_change": ("Asset Condition Change", "Monitoring infrastructure asset condition changes over time", "change_detection"),
    "beekeeping_suitability": ("Beekeeping Suitability", "Site suitability analysis for apiculture and bee habitat assessment", "classification"),
    "border_monitor": ("Border Monitor", "Border surveillance and perimeter monitoring from satellite imagery", "detection"),
    "building_detector": ("Building Detector", "Automatic building footprint detection and extraction", "detection"),
    "builtup_detector": ("Built-up Area Detector", "Urban and built-up area detection and mapping", "segmentation"),
    "business_valuation": ("Business Valuation", "Geospatial factors for commercial property and business valuation", "regression"),
    "change_detector": ("Change Detector", "General-purpose bi-temporal change detection", "change_detection"),
    "cloud_mask": ("Cloud Mask", "Cloud and cloud shadow detection and masking", "segmentation"),
    "construction_monitor": ("Construction Monitor", "Construction site progress monitoring and analysis", "change_detection"),
    "coregistration": ("Coregistration", "Image coregistration and alignment for multi-temporal analysis", "regression"),
    "corridor_monitor": ("Corridor Monitor", "Linear infrastructure corridor monitoring (pipelines, powerlines)", "detection"),
    "crop_boundary_delineation": ("Crop Boundary Delineation", "Agricultural field boundary detection and delineation", "segmentation"),
    "crop_classifier": ("Crop Classifier", "Crop type classification from multispectral imagery", "classification"),
    "crop_detector": ("Crop Detector", "Crop presence detection and mapping", "detection"),
    "crop_growth_monitor": ("Crop Growth Monitor", "Crop growth stage monitoring and phenology tracking", "regression"),
    "crop_health_assessor": ("Crop Health Assessor", "Crop health and stress assessment", "classification"),
    "damage_assessor": ("Damage Assessor", "Post-disaster damage assessment and mapping", "classification"),
    "deforestation_detector": ("Deforestation Detector", "Forest cover loss and deforestation detection", "change_detection"),
    "dem_generator": ("DEM Generator", "Digital Elevation Model generation from stereo imagery", "regression"),
    "desertification_monitor": ("Desertification Monitor", "Land degradation and desertification monitoring", "change_detection"),
    "digitization_2d": ("2D Digitization", "Automatic 2D feature digitization and vectorization", "segmentation"),
    "digitization_3d": ("3D Digitization", "3D feature extraction and modeling", "regression"),
    "disaster_management": ("Disaster Management", "Multi-hazard disaster response and management support", "classification"),
    "drought_monitor": ("Drought Monitor", "Drought condition monitoring and severity assessment", "regression"),
    "dsm_generator": ("DSM Generator", "Digital Surface Model generation", "regression"),
    "dtm_generator": ("DTM Generator", "Digital Terrain Model generation", "regression"),
    "economic_spatial_assessor": ("Economic Spatial Assessor", "Spatial economic activity and development assessment", "regression"),
    "emergency_disaster_manager": ("Emergency Disaster Manager", "Emergency response coordination and resource allocation", "classification"),
    "encroachment_detector": ("Encroachment Detector", "Illegal encroachment and land grab detection", "change_detection"),
    "energy_potential": ("Energy Potential", "Renewable energy site potential assessment", "regression"),
    "environmental_monitor": ("Environmental Monitor", "Comprehensive environmental condition monitoring", "classification"),
    "environmental_risk": ("Environmental Risk", "Environmental risk assessment and mapping", "regression"),
    "erosion_detector": ("Erosion Detector", "Soil erosion detection and severity mapping", "segmentation"),
    "evi_calculator": ("EVI Calculator", "Enhanced Vegetation Index calculation and analysis", "regression"),
    "field_surveyor": ("Field Surveyor", "Automated field survey and measurement extraction", "regression"),
    "fire_monitor": ("Fire Monitor", "Active fire detection and burn area mapping", "detection"),
    "flood_risk": ("Flood Risk", "Flood risk assessment and vulnerability mapping", "regression"),
    "flood_risk_assessor": ("Flood Risk Assessor", "Comprehensive flood hazard and risk analysis", "classification"),
    "forest_density_estimator": ("Forest Density Estimator", "Forest canopy density estimation", "regression"),
    "forest_monitor": ("Forest Monitor", "Forest health and change monitoring", "change_detection"),
    "geostatistical_analyzer": ("Geostatistical Analyzer", "Spatial statistics and geostatistical analysis", "regression"),
    "grazing_potential": ("Grazing Potential", "Pasture and grazing land suitability assessment", "regression"),
    "greenhouse_detector": ("Greenhouse Detector", "Greenhouse and plastic cover detection", "detection"),
    "ground_displacement": ("Ground Displacement", "Ground surface displacement monitoring", "regression"),
    "hazard_vulnerability": ("Hazard Vulnerability", "Multi-hazard vulnerability assessment", "regression"),
    "hydroelectric_monitor": ("Hydroelectric Monitor", "Hydroelectric infrastructure and reservoir monitoring", "change_detection"),
    "infrastructure_monitor": ("Infrastructure Monitor", "Critical infrastructure monitoring and assessment", "detection"),
    "insurance_underwriting": ("Insurance Underwriting", "Geospatial risk factors for insurance assessment", "regression"),
    "land_degradation_detector": ("Land Degradation Detector", "Land degradation detection and severity mapping", "change_detection"),
    "land_surface_temperature": ("Land Surface Temperature", "Land surface temperature estimation", "regression"),
    "landslide_risk": ("Landslide Risk", "Landslide susceptibility and risk mapping", "regression"),
    "leakage_detector": ("Leakage Detector", "Pipeline and infrastructure leakage detection", "detection"),
    "livestock_estimator": ("Livestock Estimator", "Livestock population estimation from imagery", "regression"),
    "lulc_classifier": ("LULC Classifier", "Land Use Land Cover classification", "classification"),
    "marine_pollution_detector": ("Marine Pollution Detector", "Marine pollution and oil spill detection", "detection"),
    "maritime_awareness": ("Maritime Awareness", "Maritime domain awareness and vessel monitoring", "detection"),
    "military_objects_detector": ("Military Objects Detector", "Military infrastructure and vehicle detection", "detection"),
    "mobility_analyzer": ("Mobility Analyzer", "Transportation and mobility pattern analysis", "regression"),
    "model_3d": ("3D Model", "3D scene reconstruction and modeling", "regression"),
    "mosaic_processor": ("Mosaic Processor", "Image mosaicking and seamline optimization", "regression"),
    "mosaicking": ("Mosaicking", "Large-scale image mosaic generation", "regression"),
    "msi_calculator": ("MSI Calculator", "Moisture Stress Index calculation", "regression"),
    "multi_solution_segmentation": ("Multi-Solution Segmentation", "Multi-scale image segmentation", "segmentation"),
    "multispectral_processor": ("Multispectral Processor", "Multispectral image processing and analysis", "regression"),
    "natural_resources_monitor": ("Natural Resources Monitor", "Natural resource monitoring and management", "classification"),
    "nbr_calculator": ("NBR Calculator", "Normalized Burn Ratio calculation", "regression"),
    "ndvi_calculator": ("NDVI Calculator", "Normalized Difference Vegetation Index calculation", "regression"),
    "ndwi_calculator": ("NDWI Calculator", "Normalized Difference Water Index calculation", "regression"),
    "network_analyzer": ("Network Analyzer", "Transportation and utility network analysis", "regression"),
    "object_detector": ("Object Detector", "General-purpose object detection", "detection"),
    "offshore_survey": ("Offshore Survey", "Offshore infrastructure and marine survey", "detection"),
    "onshore_monitor": ("Onshore Monitor", "Onshore oil and gas facility monitoring", "detection"),
    "ortho_processor": ("Ortho Processor", "Orthorectification and geometric correction", "regression"),
    "orthorectification": ("Orthorectification", "Image orthorectification with terrain correction", "regression"),
    "panchromatic_processor": ("Panchromatic Processor", "Panchromatic image enhancement and processing", "regression"),
    "pansharpening": ("Pansharpening", "Panchromatic and multispectral image fusion", "regression"),
    "perennial_garden_suitability": ("Perennial Garden Suitability", "Perennial crop and garden site suitability", "classification"),
    "pipeline_route_planner": ("Pipeline Route Planner", "Optimal pipeline routing and planning", "regression"),
    "pivot_inventory": ("Pivot Inventory", "Center pivot irrigation inventory and mapping", "detection"),
    "plowed_land_detector": ("Plowed Land Detector", "Plowed and tilled land detection", "detection"),
    "preparedness_manager": ("Preparedness Manager", "Disaster preparedness and planning support", "classification"),
    "protected_area_change_detector": ("Protected Area Change Detector", "Change detection in protected areas", "change_detection"),
    "raster_tiler": ("Raster Tiler", "Raster data tiling and pyramid generation", "regression"),
    "reservoir_monitor": ("Reservoir Monitor", "Reservoir water level and storage monitoring", "regression"),
    "resource_allocation": ("Resource Allocation", "Optimal resource allocation and planning", "regression"),
    "road_network_analyzer": ("Road Network Analyzer", "Road network extraction and analysis", "segmentation"),
    "route_planner": ("Route Planner", "Optimal route planning and navigation", "regression"),
    "salinity_detector": ("Salinity Detector", "Soil salinity detection and mapping", "regression"),
    "sar_amplitude": ("SAR Amplitude", "SAR amplitude image processing and analysis", "regression"),
    "sar_flood_detector": ("SAR Flood Detector", "Flood mapping from SAR imagery", "segmentation"),
    "sar_mapping_workflow": ("SAR Mapping Workflow", "End-to-end SAR processing workflow", "regression"),
    "sar_oil_spill_detector": ("SAR Oil Spill Detector", "Oil spill detection from SAR imagery", "detection"),
    "sar_phase_displacement": ("SAR Phase Displacement", "InSAR phase-based displacement measurement", "regression"),
    "sar_ship_detector": ("SAR Ship Detector", "Ship detection from SAR imagery", "detection"),
    "sar_subsidence_monitor": ("SAR Subsidence Monitor", "Ground subsidence monitoring from InSAR", "regression"),
    "savi_calculator": ("SAVI Calculator", "Soil Adjusted Vegetation Index calculation", "regression"),
    "security_monitor": ("Security Monitor", "Security and surveillance monitoring", "detection"),
    "seismic_risk": ("Seismic Risk", "Seismic hazard and risk assessment", "regression"),
    "ship_detector": ("Ship Detector", "Maritime vessel detection and tracking", "detection"),
    "site_suitability": ("Site Suitability", "General site suitability analysis", "classification"),
    "solar_site_selector": ("Solar Site Selector", "Solar energy site selection and assessment", "regression"),
    "spatial_analyzer": ("Spatial Analyzer", "General spatial analysis and modeling", "regression"),
    "spatial_relationship": ("Spatial Relationship", "Spatial relationship and topology analysis", "classification"),
    "stereo_processor": ("Stereo Processor", "Stereo image processing and 3D extraction", "regression"),
    "super_resolution": ("Super Resolution", "Image super-resolution enhancement", "regression"),
    "synthetic_imagery": ("Synthetic Imagery", "Synthetic image generation and augmentation", "regression"),
    "target_detector": ("Target Detector", "Specific target detection and identification", "detection"),
    "thematic_mapper": ("Thematic Mapper", "Thematic map generation and classification", "classification"),
    "timeseries_analyzer": ("Timeseries Analyzer", "Multi-temporal time series analysis", "regression"),
    "topography_mapper": ("Topography Mapper", "Topographic mapping and terrain analysis", "regression"),
    "tourist_destination_monitor": ("Tourist Destination Monitor", "Tourism site monitoring and impact assessment", "change_detection"),
    "transportation_mapper": ("Transportation Mapper", "Transportation infrastructure mapping", "segmentation"),
    "tree_height_estimator": ("Tree Height Estimator", "Tree and canopy height estimation", "regression"),
    "tri_stereo_processor": ("Tri-Stereo Processor", "Three-view stereo processing", "regression"),
    "urban_growth_assessor": ("Urban Growth Assessor", "Urban expansion and growth assessment", "change_detection"),
    "urban_planner": ("Urban Planner", "Urban planning and development support", "classification"),
    "utility_mapper": ("Utility Mapper", "Utility infrastructure mapping", "segmentation"),
    "vegetation_condition": ("Vegetation Condition", "Vegetation health and condition assessment", "regression"),
    "vehicle_detector": ("Vehicle Detector", "Vehicle detection and counting", "detection"),
    "viewshed_analyzer": ("Viewshed Analyzer", "Viewshed and visibility analysis", "regression"),
    "water_quality_assessor": ("Water Quality Assessor", "Water quality parameter estimation", "regression"),
    "water_surface_detector": ("Water Surface Detector", "Water body detection and mapping", "segmentation"),
    "watershed_manager": ("Watershed Manager", "Watershed delineation and management", "segmentation"),
    "wildfire_risk": ("Wildfire Risk", "Wildfire risk and spread modeling", "regression"),
    "wildlife_habitat_analyzer": ("Wildlife Habitat Analyzer", "Wildlife habitat suitability analysis", "classification"),
    "wind_site_selector": ("Wind Site Selector", "Wind energy site selection", "regression"),
    "yield_predictor": ("Yield Predictor", "Crop yield prediction and estimation", "regression"),
    "zonal_statistics": ("Zonal Statistics", "Zonal statistics computation and analysis", "regression"),
}

# Variant configurations
VARIANTS = {
    "tiny": {"resolution": 32, "channels": 16, "desc": "Ultra-lightweight for edge devices and rapid prototyping"},
    "base": {"resolution": 64, "channels": 64, "desc": "Balanced performance for production deployments"},
    "large": {"resolution": 128, "channels": 128, "desc": "High accuracy for demanding applications"},
    "mega": {"resolution": 256, "channels": 256, "desc": "Maximum precision for research and critical tasks"},
}


def create_notebook(topic: str, index: int) -> dict:
    """Create a comprehensive notebook for a model topic."""
    name, description, task = DESCRIPTIONS.get(topic, (topic.replace("_", " ").title(), f"Model for {topic}", "regression"))
    title = name
    
    colab_url = f"https://colab.research.google.com/github/unbihexium-oss/unbihexium/blob/main/examples/notebooks/{index:03d}_{topic}.ipynb"
    github_url = f"https://github.com/unbihexium-oss/unbihexium/blob/main/examples/notebooks/{index:03d}_{topic}.ipynb"
    
    notebook = {
        "cells": [
            # Header
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    f"# {title}\n",
                    "\n",
                    f"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]({colab_url})\n",
                    "[![GitHub](https://img.shields.io/badge/GitHub-View_Source-181717?logo=github)](https://github.com/unbihexium-oss/unbihexium)\n",
                    "[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n",
                    "\n",
                    "---\n",
                    "\n",
                    "**Author**: Unbihexium OSS Foundation  \n",
                    "**Version**: 1.0.0  \n",
                    "**Last Updated**: 2025-12-21  \n",
                    f"**Task Type**: `{task}`\n",
                    "\n",
                    "---\n",
                    "\n",
                    "## Table of Contents\n",
                    "\n",
                    "1. [Introduction](#1-introduction)\n",
                    "2. [Model Overview](#2-model-overview)\n",
                    "3. [Environment Setup](#3-environment-setup)\n",
                    "4. [Model Loading](#4-model-loading)\n",
                    "5. [Variant Comparison](#5-variant-comparison)\n",
                    "6. [Inference Pipeline](#6-inference-pipeline)\n",
                    "7. [Performance Benchmarks](#7-performance-benchmarks)\n",
                    "8. [Integration Examples](#8-integration-examples)\n",
                    "9. [Best Practices](#9-best-practices)\n",
                    "10. [References](#10-references)\n",
                ]
            },
            # Introduction
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 1. Introduction\n",
                    "\n",
                    f"{description}.\n",
                    "\n",
                    "This notebook provides a comprehensive guide to using the Unbihexium library's "
                    f"`{topic}` model family. The model is available in four variants (tiny, base, large, mega) "
                    "to accommodate different computational constraints and accuracy requirements.\n",
                    "\n",
                    "### Key Features\n",
                    "\n",
                    f"- **Task**: {task.replace('_', ' ').title()}\n",
                    "- **Variants**: 4 (tiny, base, large, mega)\n",
                    "- **Formats**: ONNX and PyTorch (.pt)\n",
                    "- **Framework**: Unbihexium Model Zoo\n",
                ]
            },
            # Model Overview
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 2. Model Overview\n",
                    "\n",
                    "### Use Cases\n",
                    "\n",
                    f"The `{topic}` model is designed for:\n",
                    "\n",
                    f"- Primary application: {description}\n",
                    "- Integration with geospatial workflows\n",
                    "- Batch processing of satellite imagery\n",
                    "- Real-time inference for operational systems\n",
                    "\n",
                    "### Technical Specifications\n",
                    "\n",
                    "| Variant | Resolution | Channels | Parameters | Use Case |\n",
                    "|---------|------------|----------|------------|----------|\n",
                    f"| tiny | 32x32 | 16 | ~17K | {VARIANTS['tiny']['desc']} |\n",
                    f"| base | 64x64 | 64 | ~268K | {VARIANTS['base']['desc']} |\n",
                    f"| large | 128x128 | 128 | ~1M | {VARIANTS['large']['desc']} |\n",
                    f"| mega | 256x256 | 256 | ~4M | {VARIANTS['mega']['desc']} |\n",
                ]
            },
            # Environment Setup
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 3. Environment Setup\n",
                    "\n",
                    "### Prerequisites\n",
                    "\n",
                    "- Python 3.10 or higher\n",
                    "- 8 GB RAM minimum (16 GB recommended for large/mega variants)\n",
                    "- CUDA-compatible GPU (optional, for accelerated inference)\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Install required packages\n",
                    "# Uncomment the following lines if running in a fresh environment\n",
                    "\n",
                    "# !pip install unbihexium\n",
                    "# !pip install onnxruntime  # or onnxruntime-gpu for GPU acceleration\n",
                    "# !pip install torch torchvision\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Verify installation\n",
                    "import sys\n",
                    "print(f\"Python version: {sys.version}\")\n",
                    "\n",
                    "try:\n",
                    "    import unbihexium\n",
                    "    print(f\"Unbihexium version: {unbihexium.__version__}\")\n",
                    "except ImportError:\n",
                    "    print(\"Unbihexium not installed. Run: pip install unbihexium\")\n",
                    "\n",
                    "try:\n",
                    "    import onnxruntime as ort\n",
                    "    print(f\"ONNX Runtime version: {ort.__version__}\")\n",
                    "    print(f\"Available providers: {ort.get_available_providers()}\")\n",
                    "except ImportError:\n",
                    "    print(\"ONNX Runtime not installed. Run: pip install onnxruntime\")\n",
                    "\n",
                    "try:\n",
                    "    import torch\n",
                    "    print(f\"PyTorch version: {torch.__version__}\")\n",
                    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                    "except ImportError:\n",
                    "    print(\"PyTorch not installed. Run: pip install torch\")\n",
                ]
            },
            # Model Loading
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 4. Model Loading\n",
                    "\n",
                    "### 4.1 Using Unbihexium Model Zoo\n",
                    "\n",
                    "The recommended approach is to use the Unbihexium Model Zoo API for automatic model management.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "from pathlib import Path\n",
                    "import json\n",
                    "\n",
                    "# Define model paths for all variants\n",
                    f"MODEL_ID = \"{topic}\"\n",
                    "VARIANTS = [\"tiny\", \"base\", \"large\", \"mega\"]\n",
                    "\n",
                    "# Repository root (adjust path as needed)\n",
                    "REPO_ROOT = Path(\"../..\")\n",
                    "MODEL_ZOO = REPO_ROOT / \"model_zoo\" / \"assets\"\n",
                    "\n",
                    "# Load configuration for each variant\n",
                    "configs = {}\n",
                    "for variant in VARIANTS:\n",
                    "    config_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"config.json\"\n",
                    "    if config_path.exists():\n",
                    "        with open(config_path) as f:\n",
                    "            configs[variant] = json.load(f)\n",
                    "        print(f\"{variant.upper()}: {configs[variant]}\")\n",
                    "    else:\n",
                    "        print(f\"{variant.upper()}: Configuration not found at {config_path}\")\n",
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "### 4.2 Direct ONNX Loading\n",
                    "\n",
                    "For production deployments, ONNX format provides cross-platform compatibility.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import onnxruntime as ort\n",
                    "import numpy as np\n",
                    "\n",
                    "# Load ONNX models for all variants\n",
                    "onnx_sessions = {}\n",
                    "\n",
                    "for variant in VARIANTS:\n",
                    "    model_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.onnx\"\n",
                    "    if model_path.exists():\n",
                    "        try:\n",
                    "            session = ort.InferenceSession(\n",
                    "                str(model_path),\n",
                    "                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
                    "            )\n",
                    "            onnx_sessions[variant] = session\n",
                    "            \n",
                    "            # Print model info\n",
                    "            input_info = session.get_inputs()[0]\n",
                    "            output_info = session.get_outputs()[0]\n",
                    "            print(f\"{variant.upper()} ONNX loaded:\")\n",
                    "            print(f\"  Input: {input_info.name} {input_info.shape}\")\n",
                    "            print(f\"  Output: {output_info.name} {output_info.shape}\")\n",
                    "        except Exception as e:\n",
                    "            print(f\"{variant.upper()}: Failed to load - {e}\")\n",
                    "    else:\n",
                    "        print(f\"{variant.upper()}: ONNX model not found\")\n",
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "### 4.3 PyTorch Loading\n",
                    "\n",
                    "For research and fine-tuning, PyTorch format provides full model access.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import torch\n",
                    "\n",
                    "# Load PyTorch models\n",
                    "pt_models = {}\n",
                    "\n",
                    "for variant in VARIANTS:\n",
                    "    model_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.pt\"\n",
                    "    if model_path.exists():\n",
                    "        try:\n",
                    "            model = torch.jit.load(str(model_path), map_location='cpu')\n",
                    "            model.eval()\n",
                    "            pt_models[variant] = model\n",
                    "            \n",
                    "            # Count parameters\n",
                    "            param_count = sum(p.numel() for p in model.parameters())\n",
                    "            print(f\"{variant.upper()} PyTorch loaded: {param_count:,} parameters\")\n",
                    "        except Exception as e:\n",
                    "            print(f\"{variant.upper()}: Failed to load - {e}\")\n",
                    "    else:\n",
                    "        print(f\"{variant.upper()}: PyTorch model not found\")\n",
                ]
            },
            # Variant Comparison
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 5. Variant Comparison\n",
                    "\n",
                    "Compare the four model variants across key metrics.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import os\n",
                    "\n",
                    "# Compare model sizes and configurations\n",
                    "comparison_data = []\n",
                    "\n",
                    "for variant in VARIANTS:\n",
                    "    onnx_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.onnx\"\n",
                    "    pt_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.pt\"\n",
                    "    \n",
                    "    row = {\"Variant\": variant.upper()}\n",
                    "    \n",
                    "    if onnx_path.exists():\n",
                    "        row[\"ONNX Size (MB)\"] = round(os.path.getsize(onnx_path) / (1024 * 1024), 2)\n",
                    "    if pt_path.exists():\n",
                    "        row[\"PT Size (MB)\"] = round(os.path.getsize(pt_path) / (1024 * 1024), 2)\n",
                    "    if variant in configs:\n",
                    "        row[\"Parameters\"] = configs[variant].get(\"params\", \"N/A\")\n",
                    "        row[\"Resolution\"] = configs[variant].get(\"resolution\", \"N/A\")\n",
                    "    \n",
                    "    comparison_data.append(row)\n",
                    "\n",
                    "# Display comparison table\n",
                    "try:\n",
                    "    import pandas as pd\n",
                    "    df = pd.DataFrame(comparison_data)\n",
                    "    print(df.to_string(index=False))\n",
                    "except ImportError:\n",
                    "    for row in comparison_data:\n",
                    "        print(row)\n",
                ]
            },
            # Inference Pipeline
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 6. Inference Pipeline\n",
                    "\n",
                    "### 6.1 Input Preparation\n",
                    "\n",
                    "Prepare input data according to model requirements.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def prepare_input(resolution: int, channels: int = 3, batch_size: int = 1):\n",
                    "    \"\"\"\n",
                    "    Prepare synthetic input tensor for inference.\n",
                    "    \n",
                    "    Parameters\n",
                    "    ----------\n",
                    "    resolution : int\n",
                    "        Spatial resolution (width and height)\n",
                    "    channels : int\n",
                    "        Number of input channels (default: 3 for RGB)\n",
                    "    batch_size : int\n",
                    "        Batch size for inference\n",
                    "    \n",
                    "    Returns\n",
                    "    -------\n",
                    "    np.ndarray\n",
                    "        Input tensor of shape (batch_size, channels, resolution, resolution)\n",
                    "    \"\"\"\n",
                    "    # Generate synthetic input (replace with actual data in production)\n",
                    "    return np.random.rand(batch_size, channels, resolution, resolution).astype(np.float32)\n",
                    "\n",
                    "# Resolution mapping for each variant\n",
                    "RESOLUTIONS = {\n",
                    "    \"tiny\": 32,\n",
                    "    \"base\": 64,\n",
                    "    \"large\": 128,\n",
                    "    \"mega\": 256\n",
                    "}\n",
                    "\n",
                    "# Prepare inputs for all variants\n",
                    "inputs = {}\n",
                    "for variant, res in RESOLUTIONS.items():\n",
                    "    inputs[variant] = prepare_input(res)\n",
                    "    print(f\"{variant.upper()} input shape: {inputs[variant].shape}\")\n",
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "### 6.2 ONNX Inference\n",
                    "\n",
                    "Run inference using ONNX Runtime.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Run ONNX inference for all variants\n",
                    "onnx_outputs = {}\n",
                    "\n",
                    "for variant, session in onnx_sessions.items():\n",
                    "    input_name = session.get_inputs()[0].name\n",
                    "    input_data = inputs[variant]\n",
                    "    \n",
                    "    # Run inference\n",
                    "    output = session.run(None, {input_name: input_data})\n",
                    "    onnx_outputs[variant] = output[0]\n",
                    "    \n",
                    "    print(f\"{variant.upper()} ONNX output shape: {output[0].shape}\")\n",
                    "    print(f\"  Min: {output[0].min():.4f}, Max: {output[0].max():.4f}, Mean: {output[0].mean():.4f}\")\n",
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "### 6.3 PyTorch Inference\n",
                    "\n",
                    "Run inference using PyTorch.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Run PyTorch inference for all variants\n",
                    "pt_outputs = {}\n",
                    "\n",
                    "with torch.no_grad():\n",
                    "    for variant, model in pt_models.items():\n",
                    "        input_tensor = torch.from_numpy(inputs[variant])\n",
                    "        \n",
                    "        # Run inference\n",
                    "        output = model(input_tensor)\n",
                    "        pt_outputs[variant] = output.numpy()\n",
                    "        \n",
                    "        print(f\"{variant.upper()} PyTorch output shape: {output.shape}\")\n",
                    "        print(f\"  Min: {output.min():.4f}, Max: {output.max():.4f}, Mean: {output.mean():.4f}\")\n",
                ]
            },
            # Performance Benchmarks
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 7. Performance Benchmarks\n",
                    "\n",
                    "Measure inference time for each variant and framework.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import time\n",
                    "\n",
                    "def benchmark(func, n_runs: int = 10, warmup: int = 3):\n",
                    "    \"\"\"\n",
                    "    Benchmark a function.\n",
                    "    \n",
                    "    Parameters\n",
                    "    ----------\n",
                    "    func : callable\n",
                    "        Function to benchmark\n",
                    "    n_runs : int\n",
                    "        Number of timed runs\n",
                    "    warmup : int\n",
                    "        Number of warmup runs\n",
                    "    \n",
                    "    Returns\n",
                    "    -------\n",
                    "    tuple\n",
                    "        (mean_time_ms, std_time_ms)\n",
                    "    \"\"\"\n",
                    "    # Warmup\n",
                    "    for _ in range(warmup):\n",
                    "        func()\n",
                    "    \n",
                    "    # Timed runs\n",
                    "    times = []\n",
                    "    for _ in range(n_runs):\n",
                    "        start = time.perf_counter()\n",
                    "        func()\n",
                    "        end = time.perf_counter()\n",
                    "        times.append((end - start) * 1000)  # Convert to ms\n",
                    "    \n",
                    "    return np.mean(times), np.std(times)\n",
                    "\n",
                    "# Benchmark ONNX inference\n",
                    "print(\"ONNX Runtime Performance:\")\n",
                    "print(\"-\" * 50)\n",
                    "onnx_benchmark = {}\n",
                    "for variant, session in onnx_sessions.items():\n",
                    "    input_name = session.get_inputs()[0].name\n",
                    "    input_data = inputs[variant]\n",
                    "    \n",
                    "    mean_ms, std_ms = benchmark(lambda: session.run(None, {input_name: input_data}))\n",
                    "    onnx_benchmark[variant] = mean_ms\n",
                    "    print(f\"{variant.upper():6} | {mean_ms:8.2f} ms +/- {std_ms:.2f} ms\")\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Benchmark PyTorch inference\n",
                    "print(\"PyTorch Performance:\")\n",
                    "print(\"-\" * 50)\n",
                    "pt_benchmark = {}\n",
                    "\n",
                    "with torch.no_grad():\n",
                    "    for variant, model in pt_models.items():\n",
                    "        input_tensor = torch.from_numpy(inputs[variant])\n",
                    "        \n",
                    "        mean_ms, std_ms = benchmark(lambda: model(input_tensor))\n",
                    "        pt_benchmark[variant] = mean_ms\n",
                    "        print(f\"{variant.upper():6} | {mean_ms:8.2f} ms +/- {std_ms:.2f} ms\")\n",
                ]
            },
            # Integration Examples
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 8. Integration Examples\n",
                    "\n",
                    "### 8.1 Batch Processing\n",
                    "\n",
                    "Process multiple images in a single batch for improved throughput.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def batch_inference(session, images: list, batch_size: int = 4):\n",
                    "    \"\"\"\n",
                    "    Process images in batches.\n",
                    "    \n",
                    "    Parameters\n",
                    "    ----------\n",
                    "    session : ort.InferenceSession\n",
                    "        ONNX inference session\n",
                    "    images : list\n",
                    "        List of input images (numpy arrays)\n",
                    "    batch_size : int\n",
                    "        Batch size for processing\n",
                    "    \n",
                    "    Returns\n",
                    "    -------\n",
                    "    list\n",
                    "        List of outputs\n",
                    "    \"\"\"\n",
                    "    results = []\n",
                    "    input_name = session.get_inputs()[0].name\n",
                    "    \n",
                    "    for i in range(0, len(images), batch_size):\n",
                    "        batch = np.stack(images[i:i+batch_size])\n",
                    "        output = session.run(None, {input_name: batch})\n",
                    "        results.extend(output[0])\n",
                    "    \n",
                    "    return results\n",
                    "\n",
                    "# Example: Process 8 images with the base variant\n",
                    "if \"base\" in onnx_sessions:\n",
                    "    sample_images = [prepare_input(64)[0] for _ in range(8)]\n",
                    "    batch_results = batch_inference(onnx_sessions[\"base\"], sample_images, batch_size=4)\n",
                    "    print(f\"Processed {len(sample_images)} images, got {len(batch_results)} outputs\")\n",
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "### 8.2 Model Selection Strategy\n",
                    "\n",
                    "Choose the appropriate variant based on requirements.\n",
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def select_variant(\n",
                    "    max_latency_ms: float = None,\n",
                    "    max_memory_mb: float = None,\n",
                    "    min_resolution: int = None,\n",
                    "    prefer_accuracy: bool = True\n",
                    ") -> str:\n",
                    "    \"\"\"\n",
                    "    Select the best model variant based on constraints.\n",
                    "    \n",
                    "    Parameters\n",
                    "    ----------\n",
                    "    max_latency_ms : float, optional\n",
                    "        Maximum acceptable latency in milliseconds\n",
                    "    max_memory_mb : float, optional\n",
                    "        Maximum model size in megabytes\n",
                    "    min_resolution : int, optional\n",
                    "        Minimum required input resolution\n",
                    "    prefer_accuracy : bool\n",
                    "        If True, prefer larger models when constraints allow\n",
                    "    \n",
                    "    Returns\n",
                    "    -------\n",
                    "    str\n",
                    "        Recommended variant name\n",
                    "    \"\"\"\n",
                    "    # Priority order based on preference\n",
                    "    variants = [\"mega\", \"large\", \"base\", \"tiny\"] if prefer_accuracy else [\"tiny\", \"base\", \"large\", \"mega\"]\n",
                    "    \n",
                    "    for variant in variants:\n",
                    "        # Check latency constraint\n",
                    "        if max_latency_ms and variant in onnx_benchmark:\n",
                    "            if onnx_benchmark[variant] > max_latency_ms:\n",
                    "                continue\n",
                    "        \n",
                    "        # Check resolution constraint\n",
                    "        if min_resolution and RESOLUTIONS[variant] < min_resolution:\n",
                    "            continue\n",
                    "        \n",
                    "        return variant\n",
                    "    \n",
                    "    return \"base\"  # Default fallback\n",
                    "\n",
                    "# Example usage\n",
                    "recommended = select_variant(max_latency_ms=50, prefer_accuracy=True)\n",
                    "print(f\"Recommended variant for <50ms latency: {recommended.upper()}\")\n",
                ]
            },
            # Best Practices
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 9. Best Practices\n",
                    "\n",
                    "### Variant Selection Guidelines\n",
                    "\n",
                    "| Scenario | Recommended Variant | Rationale |\n",
                    "|----------|--------------------|-----------|\n",
                    "| Edge deployment (IoT, drones) | tiny | Minimal memory and compute |\n",
                    "| Production API service | base | Balanced performance |\n",
                    "| High-accuracy batch processing | large | Better accuracy, acceptable latency |\n",
                    "| Research and validation | mega | Maximum precision |\n",
                    "\n",
                    "### Performance Optimization\n",
                    "\n",
                    "1. **Use ONNX for Production**: ONNX Runtime provides optimized execution across platforms.\n",
                    "2. **Enable GPU Acceleration**: Use `CUDAExecutionProvider` for NVIDIA GPUs.\n",
                    "3. **Batch Processing**: Increase batch size to improve GPU utilization.\n",
                    "4. **Model Caching**: Cache loaded models to avoid repeated loading overhead.\n",
                    "5. **Input Preprocessing**: Use vectorized operations for input preparation.\n",
                    "\n",
                    "### Memory Management\n",
                    "\n",
                    "```python\n",
                    "# Clear GPU memory after inference\n",
                    "import gc\n",
                    "import torch\n",
                    "\n",
                    "torch.cuda.empty_cache()\n",
                    "gc.collect()\n",
                    "```\n",
                ]
            },
            # References
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 10. References\n",
                    "\n",
                    "### Documentation\n",
                    "\n",
                    "- [Unbihexium Documentation](https://unbihexium-oss.github.io/unbihexium/)\n",
                    "- [Model Zoo Reference](https://github.com/unbihexium-oss/unbihexium/tree/main/model_zoo)\n",
                    "- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)\n",
                    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
                    "\n",
                    "### Related Notebooks\n",
                    "\n",
                    "- See the [notebooks index](./README.md) for all available tutorials.\n",
                    "\n",
                    "---\n",
                    "\n",
                    "**License**: Apache-2.0  \n",
                    "**Copyright**: 2025 Unbihexium OSS Foundation\n",
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {"name": "ipython", "version": 3},
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.12.0"
            },
            "colab": {
                "provenance": [],
                "collapsed_sections": []
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    return notebook


def main():
    """Generate all 130 notebooks."""
    output_dir = Path("examples/notebooks")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for i, topic in enumerate(TOPICS, start=1):
        notebook = create_notebook(topic, i)
        filename = output_dir / f"{i:03d}_{topic}.ipynb"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(notebook, f, indent=4, ensure_ascii=False)
        
        print(f"Created: {filename.name}")
    
    print(f"\nGenerated {len(TOPICS)} notebooks in {output_dir}")


if __name__ == "__main__":
    main()
